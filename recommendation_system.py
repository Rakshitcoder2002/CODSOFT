# -*- coding: utf-8 -*-
"""Recommendation System.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fVQFjV9ZZFv1JeGe091m6EMvFSJ-vdXP
"""

pip install pandas transformers torch google-colab

import pandas as pd
from google.colab import files
from transformers import BertTokenizer, BertModel
import torch
import numpy as np

def upload_and_read_csv():
    uploaded = files.upload()
    for filename in uploaded.keys():
        df = pd.read_csv(filename)
        print(f"Dataset '{filename}' successfully loaded.")
        print(f"Columns: {df.columns.tolist()}")
        return df

def initialize_model():
    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
    model = BertModel.from_pretrained('bert-base-uncased')
    return tokenizer, model

def encode_text(texts, tokenizer, model):
    inputs = tokenizer(texts, return_tensors='pt', padding=True, truncation=True)
    with torch.no_grad():
        outputs = model(**inputs)
    return outputs.last_hidden_state.mean(dim=1)

def generate_recommendation(query, df, embeddings, model, tokenizer):
    query_embedding = encode_text([query], tokenizer, model)
    similarities = np.dot(embeddings, query_embedding.T).flatten()
    most_similar_idx = similarities.argmax()
    if similarities[most_similar_idx] < 0.5:
        return "Sorry, I couldn't find a strong recommendation based on your query."
    best_match = df.iloc[most_similar_idx]
    return best_match.to_dict(), similarities[most_similar_idx]

def main():
    df = upload_and_read_csv()
    tokenizer, model = initialize_model()
    combined_text = df.apply(lambda row: ' '.join(row.values.astype(str)), axis=1)
    embeddings = encode_text(combined_text.tolist(), tokenizer, model)
    while True:
        user_query = input("\nWhat do you need a recommendation for? (type 'exit' to quit): ").strip()
        if user_query.lower() == 'exit':
            break
        recommendation = generate_recommendation(user_query, df, embeddings, model, tokenizer)

        if isinstance(recommendation, str):
            print(f"\n{recommendation}")
        else:
            print(f"\nAI Recommendation (Confidence {recommendation[1]:.2f}):\n{recommendation[0]}")

if __name__ == "__main__":
    main()

import pandas as pd
from google.colab import files
from transformers import BertTokenizer, BertModel
import torch
import numpy as np
import re

def upload_and_read_csv():
    uploaded = files.upload()
    for filename in uploaded.keys():
        df = pd.read_csv(filename)
        print(f"Dataset '{filename}' successfully loaded.")
        print(f"Columns: {df.columns.tolist()}")
        return df

def initialize_model():
    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
    model = BertModel.from_pretrained('bert-base-uncased')
    return tokenizer, model

def clean_text(text):
    text = re.sub(r'\s+', ' ', text)
    text = text.strip()
    return text

def encode_text(texts, tokenizer, model):
    texts = [clean_text(text) for text in texts]
    inputs = tokenizer(texts, return_tensors='pt', padding=True, truncation=True)
    with torch.no_grad():
        outputs = model(**inputs)
    return outputs.last_hidden_state.mean(dim=1)

def generate_recommendation(query, df, embeddings, model, tokenizer):
    query_embedding = encode_text([query], tokenizer, model)
    similarities = np.dot(embeddings, query_embedding.T).flatten()
    if similarities.max() < 0.5:
        return "Sorry, I couldn't find a strong recommendation based on your query."
    most_similar_idx = similarities.argmax()
    best_match = df.iloc[most_similar_idx]
    return best_match.to_dict(), similarities[most_similar_idx]

def main():
    df = upload_and_read_csv()
    tokenizer, model = initialize_model()
    combined_text = df.apply(lambda row: ' '.join(row.values.astype(str)), axis=1)
    embeddings = encode_text(combined_text.tolist(), tokenizer, model)
    while True:
        user_query = input("\nWhat do you need a recommendation for? (type 'exit' to quit): ").strip()
        if user_query.lower() == 'exit':
            break
        recommendation = generate_recommendation(user_query, df, embeddings, model, tokenizer)

        if isinstance(recommendation, str):
            print(f"\n{recommendation}")
        else:
            print(f"\nAI Recommendation (Confidence {recommendation[1]:.2f}):\n{recommendation[0]}")

if __name__ == "__main__":
    main()